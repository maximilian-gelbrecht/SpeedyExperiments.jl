{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Horizontal Diffusion \n",
    "\n",
    "As a fist example, I kind of randomly picked the horizontal diffusion struct / function. \n",
    "\n",
    "But first, we have to load the enviroment, also load a state of SpeedyWeather that we can use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Nextcloud/SpeedyExperiments/scripts`\n",
      "WARNING: using SpeedyWeather.Parameters in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "import Pkg \n",
    "Pkg.activate(\"..\")\n",
    "\n",
    "using Enzyme, Test, KernelAbstractions, CUDAKernels, KernelGradients, SpeedyExperiments, LinearAlgebra, Adapt, Parameters, SpeedyWeather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "progn_vars, diagn_vars, model_setup = initialize_speedy();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want to look into rewriting/adjusting `horizontal_diffusion!` (2D Version) here as an example. First, we will look up how this function is called given an initialized model. Looking up the source code we can see that the function signature is \n",
    "\n",
    "```julia\n",
    " horizontal_diffusion!(  tendency::AbstractMatrix{Complex{NF}}, # tendency of a \n",
    "                            A::AbstractMatrix{Complex{NF}},        # spectral horizontal field\n",
    "                            damp_expl::AbstractMatrix{NF},         # explicit spectral damping\n",
    "                            damp_impl::AbstractMatrix{NF}          # implicit spectral damping\n",
    "                            ) where {NF<:AbstractFloat}\n",
    "```\n",
    "\n",
    "and it is called in the timestepping routine with \n",
    "\n",
    "```julia\n",
    "@unpack vor = progn\n",
    "@unpack vor_tend = diagn.tendencies\n",
    "@unpack damping, damping_impl = M.horizontal_diffusion\n",
    "\n",
    "# set all tendencies to zero\n",
    "fill!(vor_tend,zero(Complex{NF}))\n",
    "\n",
    "  \n",
    "# PROPAGATE THE SPECTRAL STATE INTO THE DIAGNOSTIC VARIABLES\n",
    "gridded!(diagn,progn,M,lf2)\n",
    "\n",
    "# COMPUTE TENDENCIES OF PROGNOSTIC VARIABLES\n",
    "get_tendencies!(diagn,progn,M,lf2)                   \n",
    "\n",
    "vor_lf = view(vor,:,:,1,:)                                      # array view for leapfrog index\n",
    "horizontal_diffusion!(vor_tend,vor_lf,damping,damping_impl)     # diffusion of vorticity\n",
    "```\n",
    "\n",
    "in which `progn` is an instance of the `PrognosticVariables` like the `progn_vars` we intialized, `diagn` is an instance of `DiagnosticVariables` like the `diagn_vars` we initialized and `M` is the `model_setup`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call it once, so we can later also cross-check our new version later. We have explicitly add `SpeedyWeather` a few times as those functions are not exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = model_setup \n",
    "diagn = diagn_vars \n",
    "progn = progn_vars \n",
    "lf2 = 2\n",
    "NF = Float32\n",
    "\n",
    "@unpack vor = progn\n",
    "@unpack vor_tend = diagn.tendencies\n",
    "@unpack damping, damping_impl = M.horizontal_diffusion\n",
    "\n",
    "# set all tendencies to zero\n",
    "fill!(vor_tend,zero(Complex{NF}))\n",
    "\n",
    "# PROPAGATE THE SPECTRAL STATE INTO THE DIAGNOSTIC VARIABLES\n",
    "SpeedyWeather.gridded!(diagn,progn,M,lf2)\n",
    "\n",
    "# COMPUTE TENDENCIES OF PROGNOSTIC VARIABLES\n",
    "SpeedyWeather.get_tendencies!(diagn,progn,M,lf2)                   \n",
    "\n",
    "# we want the 2D version: \n",
    "\n",
    "vor_tend = vor_tend[:,:,1]\n",
    "vor_lf = view(vor,:,:,1,1)                                      # array view for leapfrog index\n",
    "SpeedyWeather.horizontal_diffusion!(vor_tend,vor_lf,damping,damping_impl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we know how to call the function, let's rewrite it to work on GPU and with Enzyme! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we inspect the old version: \n",
    "\n",
    "\n",
    "```julia \n",
    "function horizontal_diffusion!( tendency::AbstractMatrix{Complex{NF}}, # tendency of a \n",
    "                                A::AbstractMatrix{Complex{NF}},        # spectral horizontal field\n",
    "                                damp_expl::AbstractMatrix{NF},         # explicit spectral damping\n",
    "                                damp_impl::AbstractMatrix{NF}          # implicit spectral damping\n",
    "                                ) where {NF<:AbstractFloat}\n",
    "\n",
    "    lmax,mmax = size(A) .- 1            # degree l, order m but 0-based\n",
    "    @boundscheck size(A) == size(tendency) || throw(BoundsError())\n",
    "    @boundscheck size(A) == size(damp_expl) || throw(BoundsError())\n",
    "    @boundscheck size(A) == size(damp_impl) || throw(BoundsError())\n",
    "    \n",
    "    @inbounds for m in 1:mmax+1         # loop through all spectral modes \n",
    "        for l in m:lmax+1\n",
    "            tendency[l,m] = (tendency[l,m] - damp_expl[l,m]*A[l,m])*damp_impl[l,m]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`horizontal_diffusion!` consists of bounds checks and a double for loop. We'll have to write a kernel for the loop and then call this kernel from a wrapper function that also includes the bounds checks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to note is that all matrices that save spherical harmonics like `tendency` here, are only filled in the lower triangle. The loop also only goes over the lower triangle of the matrix, so we have make our GPU operation also only work on the lower triangle, otherwise we waste computational power. The easiest way (that I can think of) is to translate a linear index to the index of the lower triangle. So that $1\\rightarrow(1,1)$, $2\\rightarrow(2,1)$, $3\\rightarrow(2,2)$, $4\\rightarrow(3,1)$ and so on and so furth. We will have to do that all the time, so we will just create a translation array that we will reuse for all other parts of the model as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528-element Vector{CartesianIndex}:\n",
       " CartesianIndex(1, 1)\n",
       " CartesianIndex(2, 1)\n",
       " CartesianIndex(2, 2)\n",
       " CartesianIndex(3, 1)\n",
       " CartesianIndex(3, 2)\n",
       " CartesianIndex(3, 3)\n",
       " CartesianIndex(4, 1)\n",
       " CartesianIndex(4, 2)\n",
       " CartesianIndex(4, 3)\n",
       " CartesianIndex(4, 4)\n",
       " CartesianIndex(5, 1)\n",
       " CartesianIndex(5, 2)\n",
       " CartesianIndex(5, 3)\n",
       " ⋮\n",
       " CartesianIndex(32, 21)\n",
       " CartesianIndex(32, 22)\n",
       " CartesianIndex(32, 23)\n",
       " CartesianIndex(32, 24)\n",
       " CartesianIndex(32, 25)\n",
       " CartesianIndex(32, 26)\n",
       " CartesianIndex(32, 27)\n",
       " CartesianIndex(32, 28)\n",
       " CartesianIndex(32, 29)\n",
       " CartesianIndex(32, 30)\n",
       " CartesianIndex(32, 31)\n",
       " CartesianIndex(32, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    lowertriangle_indices(Lmax::Integer)\n",
    "\n",
    "Returns an array of `CartesianIndex` with the indices of the lower triangle of the square matrix with `Lmax` rows/columns\n",
    "\"\"\"\n",
    "function lowertriangle_indices(Lmax::Integer)\n",
    "    N = sum(1:Lmax) # number of elements in the lower triangle\n",
    "    indices = Array{CartesianIndex}(undef, N)\n",
    "\n",
    "    count = 1\n",
    "    for i=1:Lmax\n",
    "        for j=1:i\n",
    "            indices[count] = CartesianIndex(i,j)\n",
    "            count += 1\n",
    "        end \n",
    "    end \n",
    "    return indices \n",
    "end \n",
    "\n",
    "\"\"\"\n",
    "    lowertriangle_indices(A::AbstractMatrix)\n",
    "\n",
    "Returns an array of `CartesianIndex` with the indices of the lower triangle of the square matrix `A`.\n",
    "\"\"\"\n",
    "function lowertriangle_indices(A::AbstractMatrix) \n",
    "    @assert size(A,1) == size(A,2)\n",
    "    lowertriangle_indices(size(A,1))\n",
    "end\n",
    "\n",
    "triangle_indices = lowertriangle_indices(vor_tend)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So, now let's write the horizontal diffusion kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "horizontal_diffusion_kernel! (generic function with 5 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kernel function horizontal_diffusion_kernel!(tendency, @Const(A), @Const(damp_expl), @Const(damp_impl), @Const(triangle_index))\n",
    "    i = @index(Global, Linear)\n",
    "    i_cartesian = triangle_index[i]\n",
    "\n",
    "    tendency[i_cartesian] = (tendency[i_cartesian] - damp_expl[i_cartesian]*A[i_cartesian])*damp_impl[i_cartesian]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the bounds checks from the old version an integrate now the kernel and launch it. We might agree on some other utility functions for the kernel launching later, but here I have a struct called `DeviceSetup` that holds the currently used device and workgroup size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "horizontal_diffusion! (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function horizontal_diffusion!(tendency::AbstractMatrix{Complex{NF}}, # tendency of a \n",
    "    A::AbstractMatrix{Complex{NF}},        # spectral horizontal field\n",
    "    damp_expl::AbstractMatrix{NF},         # explicit spectral damping\n",
    "    damp_impl::AbstractMatrix{NF},          # implicit spectral damping\n",
    "    device_setup::DeviceSetup,              # device the function is executed on\n",
    "    triangle_indices::AbstractArray{CartesianIndex}   # array with the indices                  \n",
    "    ) where {NF<:AbstractFloat}\n",
    "\n",
    "lmax,mmax = size(A) .- 1            # degree l, order m but 0-based\n",
    "@boundscheck size(A) == size(tendency) || throw(BoundsError())\n",
    "@boundscheck size(A) == size(damp_expl) || throw(BoundsError())\n",
    "@boundscheck size(A) == size(damp_impl) || throw(BoundsError())\n",
    "device = device_setup.device()\n",
    "n = device_setup.n\n",
    "\n",
    "wait(horizontal_diffusion_kernel!(device, n)(tendency, A, damp_expl, damp_impl, triangle_indices, ndrange=length(triangle_indices)))\n",
    "\n",
    "end \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to test if it works! We'll compare the old version to the KernelAbstractions version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "const device_setup = DeviceSetup()\n",
    "\n",
    "vor_tend_old = deepcopy(vor_tend)\n",
    "vor_tend_new = deepcopy(vor_tend)\n",
    "\n",
    "SpeedyWeather.horizontal_diffusion!(vor_tend_old, vor_lf, damping, damping_impl)\n",
    "horizontal_diffusion!(vor_tend_new, vor_lf, damping, damping_impl, device_setup, triangle_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: vor_tend_old ≈ vor_tend_new\n",
       "   Evaluated: ComplexF32[0.0f0 + 0.0f0im 0.0f0 + 0.0f0im … 0.0f0 + 0.0f0im 0.0f0 + 0.0f0im; 9.214844f-7 + 0.0f0im 1.1242739f-14 + 9.101589f-15im … 0.0f0 + 0.0f0im 0.0f0 + 0.0f0im; … ; 8.1235026f-5 + 0.0f0im -6.751766f-7 + 1.2070599f-5im … -9.633636f-7 + 9.024164f-6im 0.0f0 + 0.0f0im; 0.87312204f0 + 0.0f0im -2.1297915f-6 - 1.4817448f-5im … -4.4587105f-6 + 3.97241f-5im 1.017148f-5 - 1.3497451f-5im] ≈ ComplexF32[0.0f0 + 0.0f0im 0.0f0 + 0.0f0im … 0.0f0 + 0.0f0im 0.0f0 + 0.0f0im; 9.214844f-7 + 0.0f0im 1.1242739f-14 + 9.101589f-15im … 0.0f0 + 0.0f0im 0.0f0 + 0.0f0im; … ; 8.1235026f-5 + 0.0f0im -6.751766f-7 + 1.2070599f-5im … -9.633636f-7 + 9.024164f-6im 0.0f0 + 0.0f0im; 0.87312204f0 + 0.0f0im -2.1297915f-6 - 1.4817448f-5im … -4.4587105f-6 + 3.97241f-5im 1.017148f-5 - 1.3497451f-5im]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test vor_tend_old ≈ vor_tend_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! \n",
    "\n",
    "Next step, is to bring Enzyme in and show that it is differentiable. For this we'll have to allocate the shadow memory that stores the gradient information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "∂vor_tend = fill!(similar(vor_tend), 1)\n",
    "vor_lf = Array(vor_lf) # needs to be an array not a subarray\n",
    "∂vor_lf = zero(vor_lf)\n",
    "∂damping = zero(damping)\n",
    "∂damping_impl = zero(damping_impl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix{ComplexF32} (alias for Array{Complex{Float32}, 2})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(vor_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "TaskFailedException\n\n\u001b[91m    nested task error: \u001b[39mReturn type inferred to be Union{}. Giving up.\n    Stacktrace:\n     [1] \u001b[0m\u001b[1merror\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90ms\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4merror.jl:33\u001b[24m\u001b[39m\n     [2] \u001b[0m\u001b[1mautodiff_deferred\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mtypeof(cpu_horizontal_diffusion_kernel!), ::\u001b[0mType\u001b[90m{Const}\u001b[39m, ::\u001b[0mKernelAbstractions.CompilerMetadata\u001b[90m{KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.NoDynamicCheck, CartesianIndex{1}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, KernelAbstractions.NDIteration.NDRange{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}}\u001b[39m, ::\u001b[0mVararg\u001b[90m{Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[35mEnzyme\u001b[39m \u001b[90m~/.julia/packages/Enzyme/di3zM/src/\u001b[39m\u001b[90m\u001b[4mEnzyme.jl:440\u001b[24m\u001b[39m\n     [3] \u001b[0m\u001b[1m(::KernelGradients.var\"#df#1\"{typeof(cpu_horizontal_diffusion_kernel!), typeof(cpu_horizontal_diffusion_kernel!)})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mKernelAbstractions.CompilerMetadata\u001b[90m{KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.NoDynamicCheck, CartesianIndex{1}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, KernelAbstractions.NDIteration.NDRange{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}}\u001b[39m, ::\u001b[0mDuplicated\u001b[90m{Matrix{ComplexF32}}\u001b[39m, ::\u001b[0mVararg\u001b[90m{Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[36mKernelGradients\u001b[39m \u001b[90m~/.julia/packages/KernelGradients/LqkqJ/src/\u001b[39m\u001b[90m\u001b[4mKernelGradients.jl:9\u001b[24m\u001b[39m\n     [4] \u001b[0m\u001b[1m__thread_run\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtid\u001b[39m::\u001b[0mInt64, \u001b[90mlen\u001b[39m::\u001b[0mInt64, \u001b[90mrem\u001b[39m::\u001b[0mInt64, \u001b[90mobj\u001b[39m::\u001b[0mKernelAbstractions.Kernel\u001b[90m{CPU, KernelAbstractions.NDIteration.StaticSize{(4,)}, KernelAbstractions.NDIteration.DynamicSize, KernelGradients.var\"#df#1\"{typeof(cpu_horizontal_diffusion_kernel!), typeof(cpu_horizontal_diffusion_kernel!)}}\u001b[39m, \u001b[90mndrange\u001b[39m::\u001b[0mTuple\u001b[90m{Int64}\u001b[39m, \u001b[90miterspace\u001b[39m::\u001b[0mKernelAbstractions.NDIteration.NDRange\u001b[90m{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}\u001b[39m, \u001b[90margs\u001b[39m::\u001b[0mTuple\u001b[90m{Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{Float32}}, Duplicated{Matrix{Float32}}, Const{DeviceSetup{DataType, Int64}}, Const{Vector{CartesianIndex}}}\u001b[39m, \u001b[90mdynamic\u001b[39m::\u001b[0mKernelAbstractions.NDIteration.NoDynamicCheck\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[32mKernelAbstractions\u001b[39m \u001b[90m~/.julia/packages/KernelAbstractions/1ZLga/src/\u001b[39m\u001b[90m\u001b[4mcpu.jl:157\u001b[24m\u001b[39m\n     [5] \u001b[0m\u001b[1m__run\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mobj\u001b[39m::\u001b[0mKernelAbstractions.Kernel\u001b[90m{CPU, KernelAbstractions.NDIteration.StaticSize{(4,)}, KernelAbstractions.NDIteration.DynamicSize, KernelGradients.var\"#df#1\"{typeof(cpu_horizontal_diffusion_kernel!), typeof(cpu_horizontal_diffusion_kernel!)}}\u001b[39m, \u001b[90mndrange\u001b[39m::\u001b[0mTuple\u001b[90m{Int64}\u001b[39m, \u001b[90miterspace\u001b[39m::\u001b[0mKernelAbstractions.NDIteration.NDRange\u001b[90m{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}\u001b[39m, \u001b[90margs\u001b[39m::\u001b[0mTuple\u001b[90m{Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{Float32}}, Duplicated{Matrix{Float32}}, Const{DeviceSetup{DataType, Int64}}, Const{Vector{CartesianIndex}}}\u001b[39m, \u001b[90mdynamic\u001b[39m::\u001b[0mKernelAbstractions.NDIteration.NoDynamicCheck\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[32mKernelAbstractions\u001b[39m \u001b[90m~/.julia/packages/KernelAbstractions/1ZLga/src/\u001b[39m\u001b[90m\u001b[4mcpu.jl:130\u001b[24m\u001b[39m\n     [6] \u001b[0m\u001b[1m(::KernelAbstractions.var\"#19#20\"{Nothing, Nothing, typeof(KernelAbstractions.__run), Tuple{KernelAbstractions.Kernel{CPU, KernelAbstractions.NDIteration.StaticSize{(4,)}, KernelAbstractions.NDIteration.DynamicSize, KernelGradients.var\"#df#1\"{typeof(cpu_horizontal_diffusion_kernel!), typeof(cpu_horizontal_diffusion_kernel!)}}, Tuple{Int64}, KernelAbstractions.NDIteration.NDRange{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}, Tuple{Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{Float32}}, Duplicated{Matrix{Float32}}, Const{DeviceSetup{DataType, Int64}}, Const{Vector{CartesianIndex}}}, KernelAbstractions.NDIteration.NoDynamicCheck}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[32mKernelAbstractions\u001b[39m \u001b[90m~/.julia/packages/KernelAbstractions/1ZLga/src/\u001b[39m\u001b[90m\u001b[4mcpu.jl:22\u001b[24m\u001b[39m",
     "output_type": "error",
     "traceback": [
      "TaskFailedException\n\n\u001b[91m    nested task error: \u001b[39mReturn type inferred to be Union{}. Giving up.\n    Stacktrace:\n     [1] \u001b[0m\u001b[1merror\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90ms\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4merror.jl:33\u001b[24m\u001b[39m\n     [2] \u001b[0m\u001b[1mautodiff_deferred\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mtypeof(cpu_horizontal_diffusion_kernel!), ::\u001b[0mType\u001b[90m{Const}\u001b[39m, ::\u001b[0mKernelAbstractions.CompilerMetadata\u001b[90m{KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.NoDynamicCheck, CartesianIndex{1}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, KernelAbstractions.NDIteration.NDRange{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}}\u001b[39m, ::\u001b[0mVararg\u001b[90m{Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[35mEnzyme\u001b[39m \u001b[90m~/.julia/packages/Enzyme/di3zM/src/\u001b[39m\u001b[90m\u001b[4mEnzyme.jl:440\u001b[24m\u001b[39m\n     [3] \u001b[0m\u001b[1m(::KernelGradients.var\"#df#1\"{typeof(cpu_horizontal_diffusion_kernel!), typeof(cpu_horizontal_diffusion_kernel!)})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mKernelAbstractions.CompilerMetadata\u001b[90m{KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.NoDynamicCheck, CartesianIndex{1}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, KernelAbstractions.NDIteration.NDRange{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}}\u001b[39m, ::\u001b[0mDuplicated\u001b[90m{Matrix{ComplexF32}}\u001b[39m, ::\u001b[0mVararg\u001b[90m{Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[36mKernelGradients\u001b[39m \u001b[90m~/.julia/packages/KernelGradients/LqkqJ/src/\u001b[39m\u001b[90m\u001b[4mKernelGradients.jl:9\u001b[24m\u001b[39m\n     [4] \u001b[0m\u001b[1m__thread_run\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtid\u001b[39m::\u001b[0mInt64, \u001b[90mlen\u001b[39m::\u001b[0mInt64, \u001b[90mrem\u001b[39m::\u001b[0mInt64, \u001b[90mobj\u001b[39m::\u001b[0mKernelAbstractions.Kernel\u001b[90m{CPU, KernelAbstractions.NDIteration.StaticSize{(4,)}, KernelAbstractions.NDIteration.DynamicSize, KernelGradients.var\"#df#1\"{typeof(cpu_horizontal_diffusion_kernel!), typeof(cpu_horizontal_diffusion_kernel!)}}\u001b[39m, \u001b[90mndrange\u001b[39m::\u001b[0mTuple\u001b[90m{Int64}\u001b[39m, \u001b[90miterspace\u001b[39m::\u001b[0mKernelAbstractions.NDIteration.NDRange\u001b[90m{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}\u001b[39m, \u001b[90margs\u001b[39m::\u001b[0mTuple\u001b[90m{Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{Float32}}, Duplicated{Matrix{Float32}}, Const{DeviceSetup{DataType, Int64}}, Const{Vector{CartesianIndex}}}\u001b[39m, \u001b[90mdynamic\u001b[39m::\u001b[0mKernelAbstractions.NDIteration.NoDynamicCheck\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[32mKernelAbstractions\u001b[39m \u001b[90m~/.julia/packages/KernelAbstractions/1ZLga/src/\u001b[39m\u001b[90m\u001b[4mcpu.jl:157\u001b[24m\u001b[39m\n     [5] \u001b[0m\u001b[1m__run\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mobj\u001b[39m::\u001b[0mKernelAbstractions.Kernel\u001b[90m{CPU, KernelAbstractions.NDIteration.StaticSize{(4,)}, KernelAbstractions.NDIteration.DynamicSize, KernelGradients.var\"#df#1\"{typeof(cpu_horizontal_diffusion_kernel!), typeof(cpu_horizontal_diffusion_kernel!)}}\u001b[39m, \u001b[90mndrange\u001b[39m::\u001b[0mTuple\u001b[90m{Int64}\u001b[39m, \u001b[90miterspace\u001b[39m::\u001b[0mKernelAbstractions.NDIteration.NDRange\u001b[90m{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}\u001b[39m, \u001b[90margs\u001b[39m::\u001b[0mTuple\u001b[90m{Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{Float32}}, Duplicated{Matrix{Float32}}, Const{DeviceSetup{DataType, Int64}}, Const{Vector{CartesianIndex}}}\u001b[39m, \u001b[90mdynamic\u001b[39m::\u001b[0mKernelAbstractions.NDIteration.NoDynamicCheck\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[32mKernelAbstractions\u001b[39m \u001b[90m~/.julia/packages/KernelAbstractions/1ZLga/src/\u001b[39m\u001b[90m\u001b[4mcpu.jl:130\u001b[24m\u001b[39m\n     [6] \u001b[0m\u001b[1m(::KernelAbstractions.var\"#19#20\"{Nothing, Nothing, typeof(KernelAbstractions.__run), Tuple{KernelAbstractions.Kernel{CPU, KernelAbstractions.NDIteration.StaticSize{(4,)}, KernelAbstractions.NDIteration.DynamicSize, KernelGradients.var\"#df#1\"{typeof(cpu_horizontal_diffusion_kernel!), typeof(cpu_horizontal_diffusion_kernel!)}}, Tuple{Int64}, KernelAbstractions.NDIteration.NDRange{1, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.StaticSize{(4,)}, CartesianIndices{1, Tuple{Base.OneTo{Int64}}}, Nothing}, Tuple{Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{ComplexF32}}, Duplicated{Matrix{Float32}}, Duplicated{Matrix{Float32}}, Const{DeviceSetup{DataType, Int64}}, Const{Vector{CartesianIndex}}}, KernelAbstractions.NDIteration.NoDynamicCheck}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[32mKernelAbstractions\u001b[39m \u001b[90m~/.julia/packages/KernelAbstractions/1ZLga/src/\u001b[39m\u001b[90m\u001b[4mcpu.jl:22\u001b[24m\u001b[39m",
      "",
      "Stacktrace:",
      " [1] wait",
      "   @ ./task.jl:334 [inlined]",
      " [2] wait",
      "   @ ~/.julia/packages/KernelAbstractions/1ZLga/src/cpu.jl:65 [inlined]",
      " [3] wait (repeats 2 times)",
      "   @ ~/.julia/packages/KernelAbstractions/1ZLga/src/cpu.jl:29 [inlined]",
      " [4] top-level scope",
      "   @ In[11]:2",
      " [5] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [6] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "∇horizontal_diffusion! = autodiff(horizontal_diffusion_kernel!(device_setup.device(), device_setup.n))\n",
    "wait(∇horizontal_diffusion!(Duplicated(vor_tend, ∂vor_tend), Duplicated(vor_lf,∂vor_lf), Duplicated(damping,∂damping), Duplicated(damping_impl,∂damping_impl), Const(device_setup), Const(triangle_indices); ndrange=length(triangle_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
